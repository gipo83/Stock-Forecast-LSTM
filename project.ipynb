
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

DATASETS_DIR = '.\\stock_dataset\\datasets'
DATASETS_DIR = ''
DATASETS_FILE_NAMES = {

    "stocks_2006_2018": "all_stocks_2006-01-01_to_2018-01-01.csv"

}

CHK_NONE = 0
CHK_DROP = 1
CHK_FILL = 2

PRE_NORMALIZE = 1
PRE_INCLUDE_DIFF_HIGH_LOW = 2
PRE_INCLUDE_DIFF_CLOSE_OPEN = 4
PRE_INCLUDE_LR = 8
PRE_INCLUDE_TI = 16

NORM_MIN_MAX = 0
NORM_Z_SCORE = 1


def load_dataset(dataset_name=list(DATASETS_FILE_NAMES.keys())[0],  stock_name=None, split_year_train_test=None):

    dataset_file_name = DATASETS_FILE_NAMES[dataset_name]
    dataset = pd.read_csv(os.path.join(DATASETS_DIR, dataset_file_name), index_col='Date', parse_dates=['Date'])

    if stock_name is not None:

        dataset = dataset[dataset["Name"] == stock_name]

    if split_year_train_test is not None:

        train, test = dataset[:split_year_train_test], dataset[str(int(split_year_train_test) + 1):]
        return train, test, dataset.rename(columns=lambda name: name + stock_name)

    else:

        return dataset.rename(columns=lambda name: name + "_" + stock_name)



def plot_stock(dataset, stock_name, variable_name="Open", split=(3, 1, 1)):

    start_year = dataset.index[0].year
    end_year = dataset.index[-1].year + 1
    dataset = dataset[dataset["Name"] == stock_name]

    plt.figure()
    dataset[variable_name].plot(figsize=(16, 4), legend=True)

    plt.legend()
    plt.title('{} Stock price'.format(stock_name))
    plt.show()

    train = split[0]
    val = split[1]
    test = split[2]

    for year in range(start_year, end_year - train - val - test + 1):

        plt.figure()

        dataset[str(year):str(year + train - 1)][variable_name].plot(figsize=(16, 4), legend=True)
        dataset[str(year + train):str(year + train + val - 1)][variable_name].plot(figsize=(16, 4), legend=True)
        dataset[str(year + train + val):str(year + train + val + test - 1)][variable_name].plot(figsize=(16, 4), legend=True)

        plt.legend()
        plt.title('{} Stock price - split {} / {} / {}'.format(stock_name, year, year + train, year + train + val))
        plt.show()


def check_dataset(dataset, stock_name):

    open_dataset = dataset["Open_{}".format(stock_name)].values
    close_dataset = dataset["Close_{}".format(stock_name)].values
    low_dataset = dataset["Low_{}".format(stock_name)].values
    high_dataset = dataset["High_{}".format(stock_name)].values

    print("Open values length: " + str(len(open_dataset)))
    print("Close values length: " + str(len(close_dataset)))
    print("Low values length: " + str(len(low_dataset)))
    print("High values length: " + str(len(high_dataset)))

    print()
    high_low_check_dataset = (high_dataset >= low_dataset)
    print("[High low check] - Following rows have problems: ")
    print(dataset[[not x for x in high_low_check_dataset]])
    high_low_check_dataset = np.all(high_low_check_dataset)

    print()
    high_open_check_dataset = (high_dataset >= open_dataset)
    print("[High open check] - Following rows have problems: ")
    print(dataset[[not x for x in high_open_check_dataset]])
    high_open_check_dataset = np.all(high_open_check_dataset)

    print()
    high_close_check_dataset = (high_dataset >= close_dataset)
    print("[High close check] - Following rows have problems: ")
    print(dataset[[not x for x in high_close_check_dataset]])
    high_close_check_dataset = np.all(high_close_check_dataset)

    print()
    low_open_check_dataset = (low_dataset <= open_dataset)
    print("[Low open check] - Following rows have problems: ")
    print(dataset[[not x for x in low_open_check_dataset]])
    low_open_check_dataset = np.all(low_open_check_dataset)

    print()
    low_close_check_dataset = (low_dataset <= close_dataset)
    print("[Low close check] - Following rows have problems: ")
    print(dataset[[not x for x in low_close_check_dataset]])
    low_close_check_dataset = np.all(low_close_check_dataset)
    print()

    high_low_check = high_low_check_dataset
    high_open_check = high_open_check_dataset
    high_close_check = high_close_check_dataset
    low_open_check = low_open_check_dataset
    low_close_check = low_close_check_dataset
    final_check = high_low_check and high_open_check and high_close_check and low_open_check and low_close_check

    print("Dataset correct: " + str(final_check))
    print("High low:" + str(high_low_check))
    print("High open:" + str(high_open_check))
    print("High close:" + str(high_close_check))
    print("Low open:" + str(low_open_check))
    print("Low close:" + str(low_close_check))

    return final_check


def repair_dataset(dataset, nafix=CHK_NONE):

    if nafix == CHK_DROP:

        dataset = dataset.dropna()

    elif nafix == CHK_FILL:

        dataset = dataset.fillna(method='ffill').fillna(method='bfill')

    return dataset


def get_sequences(arr, window, padding=[]):

    if len(padding) > 0:

        arr = np.vstack((padding, arr))

    y = arr[window:, 0]

    shape = (arr.shape[0] - window, window, arr.shape[1])
    strides = arr.strides[:-1] + arr.strides
    x = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)

    return x, y


def rolling_window(a, window, include_last=False):
    tmp = -1

    if include_last:
        tmp = 0

    padding = np.zeros(window - (1 + tmp))
    a = np.hstack((padding, a))
    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
    strides = a.strides + (a.strides[-1],)

    if include_last:

        res = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)

    else:

        res = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[:-1]

    return res



def pre_processing(dataset, rem_features=[], lookback=None, split=(3, 1, 1), options=0, label='Close', norm_options={"METHOD": NORM_MIN_MAX, "HIGH_LOW": (0, 1)}):

    def ema(arr, window):

        alpha = 2 / (window + 1)
        sma = np.average(arr[:window])

        ema = np.zeros(np.size(arr))
        ema[0] = arr[0] * alpha + sma * (1 - alpha)

        for i in range(1, len(arr)):

            ema[i] = arr[i] * alpha + ema[i - 1] * (1 - alpha)

        return ema


    window = 10
    EPS = np.finfo('float32').eps

    Ot = dataset["Open"].values
    Ht = dataset["High"].values
    Ct = dataset["Close"].values
    Lt = dataset["Low"].values
    V = dataset["Volume"].values
    HH_n = dataset["High"].rolling(window=window, min_periods=1).max()
    LL_n = dataset["Low"].rolling(window=window, min_periods=1).min()

    if PRE_INCLUDE_TI & options:

        # EMA
        dataset["EMA"] = ema(Ct, window)

        # STOCHASTIC
        dataset["Stochastic"] = ((Ct - LL_n) / (HH_n - LL_n + EPS)) * 100

        # ROC
        close_n = np.roll(Ct, window)
        close_n[0:window] = close_n[window]
        dataset["ROC"] = ((Ct - close_n) / (close_n + EPS)) * 100

        # RSI
        sum_gain = dataset["Close"].diff(periods=-1).rolling(window=window, min_periods=1).apply(func=lambda x: np.sum(x[x < 0]), raw=True) * -1
        sum_loss = dataset["Close"].diff(periods=-1).rolling(window=window, min_periods=1).apply(func=lambda x: np.sum(x[x >= 0]), raw=True)

        dataset["RSI"] = 100 - (100 / (1 + (sum_gain / (sum_loss + EPS)) + EPS))

        # AccDO
        dataset["AccDO"] = (((Ct - LL_n) - (HH_n - Ct)) / (HH_n - LL_n + EPS)) * V

        # MACD
        ema12 = ema(Ct, 12)
        ema26 = ema(Ct, 26)
        dataset["MACD"] = ema12 - ema26

        # Williams
        dataset["Williams"] = ((HH_n - Ct) / (HH_n - LL_n + EPS)) * 100

        # Disparity 5
        sma = dataset["Close"].rolling(window=5, min_periods=1).mean()
        Disp5 = (Ct / (sma + EPS)) * 100
        dataset["Disp5"] = Disp5

        # Disparity 10
        sma = dataset["Close"].rolling(window=10, min_periods=1).mean()
        Disp10 = (Ct / (sma + EPS)) * 100
        dataset["Disp10"] = Disp10

    if PRE_INCLUDE_LR & options:

        lr = ((Ct - Ot) / Ot)
        dataset["LR"] = lr
        lr = rolling_window(lr,window)
        for i in range(lr.shape[1]):
            dataset["LR_{}".format(i)] = lr[:,i]


    if PRE_INCLUDE_DIFF_HIGH_LOW & options:

        diff_minmax_train = Ht - Lt
        dataset["DiffHighLow"] = diff_minmax_train

    if PRE_INCLUDE_DIFF_CLOSE_OPEN & options:

        diff_minmax_train = Ct - Ot
        dataset["DiffCloseOpen"] = diff_minmax_train

    #if PRE_CLASSIFICATION & option:

     #   trade = dataset["Close"].diff(periods=-1) > 2

    if "Name" not in rem_features:
        rem_features.append("Name")

    for feature in rem_features:

        if feature != label:

            del dataset[feature]

        else:

            print("Warning!\n Cannot delete label {} column.".format(label))

    y = dataset[label]
    del dataset[label]
    dataset.insert(0, label, y)

    start_year = dataset.index[0].year
    end_year = dataset.index[-1].year + 1

    train = split[0]
    val = split[1]
    test = split[2]

    walks = {}
    n_walks = 0

    for year in range(start_year, end_year - train - val - test + 1):

        walks['WALK_{}'.format(n_walks)] = {}

        walks['WALK_{}'.format(n_walks)]['TRAIN'] = dataset[str(year):str(year + train - 1)]
        walks['WALK_{}'.format(n_walks)]['VALIDATION'] = dataset[str(year + train):str(year + train + val - 1)]
        walks['WALK_{}'.format(n_walks)]['TEST'] = dataset[str(year + train + val):str(year + train + val + test - 1)]

        n_walks += 1

    walks['N_WALKS'] = n_walks

    if PRE_NORMALIZE & options:

        for walk in range(n_walks):

            train = walks['WALK_{}'.format(walk)]['TRAIN']
            validation = walks['WALK_{}'.format(walk)]['VALIDATION']
            test = walks['WALK_{}'.format(walk)]['TEST']

            train_stats = train.describe()
            walks['WALK_{}'.format(walk)]['STD_PARAMS'] = {}
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['MEAN'] = train_stats.iloc[1, :].values
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['STD'] = train_stats.iloc[2, :].values
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['MIN'] = train_stats.iloc[3, :].values
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['MAX'] = train_stats.iloc[7, :].values

            if norm_options["METHOD"] == NORM_MIN_MAX:

                low = norm_options["HIGH_LOW"][0]
                high = norm_options["HIGH_LOW"][1]

                train = ((high - low) * (train.values - train_stats.iloc[3, :].values) / (train_stats.iloc[7, :].values - train_stats.iloc[3, :].values)) + low
                validation = ((high - low) * (validation.values - train_stats.iloc[3, :].values) / (train_stats.iloc[7, :].values - train_stats.iloc[3, :].values)) + low
                test = ((high - low) * (test.values - train_stats.iloc[3, :].values) / (train_stats.iloc[7, :].values - train_stats.iloc[3, :].values)) + low

            elif norm_options["METHOD"] == NORM_Z_SCORE:

                train = (train.values - train_stats.iloc[1, :].values) / train_stats.iloc[2, :].values
                validation = (validation.values - train_stats.iloc[1, :].values) / train_stats.iloc[2, :].values
                test = (test.values - train_stats.iloc[1, :].values) / train_stats.iloc[2, :].values

            walks['WALK_{}'.format(walk)]['TRAIN'] = get_sequences(arr=train, window=lookback)
            padding = train[-lookback:]

            walks['WALK_{}'.format(walk)]['VALIDATION'] = get_sequences(arr=validation, window=lookback, padding=padding)
            if len(validation) > 0:
                padding = validation[:-lookback]

            walks['WALK_{}'.format(walk)]['TEST'] = get_sequences(arr=test, window=lookback, padding=padding)

    else:

        for walk in range(n_walks):

            train = walks['WALK_{}'.format(walk)]['TRAIN'].values
            walks['WALK_{}'.format(walk)]['TRAIN'] = get_sequences(arr=train, window=lookback)
            padding = train[:-lookback]

            validation = walks['WALK_{}'.format(walk)]['VALIDATION'].values
            walks['WALK_{}'.format(walk)]['VALIDATION'] = get_sequences(arr=validation, window=lookback, padding=padding)
            if len(validation) > 0:
                padding = validation[:-lookback]

            test = walks['WALK_{}'.format(walk)]['TEST'].values
            walks['WALK_{}'.format(walk)]['TEST'] = get_sequences(arr=test, window=lookback, padding=padding)

    return walks


def pre_processing_last(stock_name_list, dataset, rem_features=[], lookback=None, split=(3, 1, 1), options=0, label='Close', norm_options={"METHOD": NORM_MIN_MAX, "HIGH_LOW": (0, 1)}):

    def ema(arr, window):

        alpha = 2 / (window + 1)
        sma = np.average(arr[:window])

        ema = np.zeros(np.size(arr))
        ema[0] = arr[0] * alpha + sma * (1 - alpha)

        for i in range(1, len(arr)):

            ema[i] = arr[i] * alpha + ema[i - 1] * (1 - alpha)

        return ema


    window = 10
    EPS = np.finfo('float32').eps

    for stock_name in stock_name_list:

        Ot = dataset["Open_{}".format(stock_name)].values
        Ht = dataset["High_{}".format(stock_name)].values
        Ct = dataset["Close_{}".format(stock_name)].values
        Lt = dataset["Low_{}".format(stock_name)].values
        V = dataset["Volume_{}".format(stock_name)].values
        HH_n = dataset["High_{}".format(stock_name)].rolling(window=window, min_periods=1).max()
        LL_n = dataset["Low_{}".format(stock_name)].rolling(window=window, min_periods=1).min()

        if PRE_INCLUDE_TI & options:

            # EMA
            dataset["EMA_{}".format(stock_name)] = ema(Ct, window)

            # STOCHASTIC
            dataset["Stochastic_{}".format(stock_name)] = ((Ct - LL_n) / (HH_n - LL_n + EPS)) * 100

            # ROC
            close_n = np.roll(Ct, window)
            close_n[0:window] = close_n[window]
            dataset["ROC_{}".format(stock_name)] = ((Ct - close_n) / (close_n + EPS)) * 100

            # RSI
            sum_gain = dataset["Close_{}".format(stock_name)].diff(periods=-1).rolling(window=window, min_periods=1).apply(func=lambda x: np.sum(x[x < 0]), raw=True) * -1
            sum_loss = dataset["Close_{}".format(stock_name)].diff(periods=-1).rolling(window=window, min_periods=1).apply(func=lambda x: np.sum(x[x >= 0]), raw=True)

            dataset["RSI_{}".format(stock_name)] = 100 - (100 / (1 + (sum_gain / (sum_loss + EPS)) + EPS))

            # AccDO
            dataset["AccDO_{}".format(stock_name)] = (((Ct - LL_n) - (HH_n - Ct)) / (HH_n - LL_n + EPS)) * V

            # MACD
            ema12 = ema(Ct, 12)
            ema26 = ema(Ct, 26)
            dataset["MACD_{}".format(stock_name)] = ema12 - ema26

            # Williams
            dataset["Williams_{}".format(stock_name)] = ((HH_n - Ct) / (HH_n - LL_n + EPS)) * 100

            # Disparity 5
            sma = dataset["Close_{}".format(stock_name)].rolling(window=5, min_periods=1).mean()
            Disp5 = (Ct / (sma + EPS)) * 100
            dataset["Disp5_{}".format(stock_name)] = Disp5

            # Disparity 10
            sma = dataset["Close_{}".format(stock_name)].rolling(window=10, min_periods=1).mean()
            Disp10 = (Ct / (sma + EPS)) * 100
            dataset["Disp10_{}".format(stock_name)] = Disp10

        if PRE_INCLUDE_LR & options:

            lr = ((Ct - Ot) / Ot)
            dataset["LR_{}".format(stock_name)] = lr
            #lr = rolling_window(lr,window)
            #for i in range(lr.shape[1]):
            #    dataset["LR_{}_{}".format(i,stock_name)] = lr[:,i]


        if PRE_INCLUDE_DIFF_HIGH_LOW & options:

            diff_minmax_train = Ht - Lt
            dataset["DiffHighLow_{}".format(stock_name)] = diff_minmax_train

        if PRE_INCLUDE_DIFF_CLOSE_OPEN & options:

            diff_minmax_train = Ct - Ot
            dataset["DiffCloseOpen_{}".format(stock_name)] = diff_minmax_train

        #if PRE_CLASSIFICATION & option:

         #   trade = dataset["Close"].diff(periods=-1) > 2

        if "Name" not in rem_features:
            rem_features.append("Name")

        for feature in rem_features:

            if feature != label:

                del dataset[feature+"_{}".format(stock_name)]

            else:

                print("Warning!\n Cannot delete label {} column.".format(label))


    y = dataset[label]
    del dataset[label]
    dataset.insert(0, label, y)

    start_year = dataset.index[0].year
    end_year = dataset.index[-1].year + 1

    train = split[0]
    val = split[1]
    test = split[2]

    walks = {}
    n_walks = 0

    for year in range(start_year, end_year - train - val - test + 1):

        walks['WALK_{}'.format(n_walks)] = {}

        walks['WALK_{}'.format(n_walks)]['TRAIN'] = dataset[str(year):str(year + train - 1)]
        walks['WALK_{}'.format(n_walks)]['VALIDATION'] = dataset[str(year + train):str(year + train + val - 1)]
        walks['WALK_{}'.format(n_walks)]['TEST'] = dataset[str(year + train + val):str(year + train + val + test - 1)]

        n_walks += 1

    walks['N_WALKS'] = n_walks

    if PRE_NORMALIZE & options:

        for walk in range(n_walks):

            train = walks['WALK_{}'.format(walk)]['TRAIN']
            validation = walks['WALK_{}'.format(walk)]['VALIDATION']
            test = walks['WALK_{}'.format(walk)]['TEST']

            train_stats = train.describe()
            walks['WALK_{}'.format(walk)]['STD_PARAMS'] = {}
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['MEAN'] = train_stats.iloc[1, :].values
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['STD'] = train_stats.iloc[2, :].values
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['MIN'] = train_stats.iloc[3, :].values
            walks['WALK_{}'.format(walk)]['STD_PARAMS']['MAX'] = train_stats.iloc[7, :].values

            if norm_options["METHOD"] == NORM_MIN_MAX:

                low = norm_options["HIGH_LOW"][0]
                high = norm_options["HIGH_LOW"][1]

                train = ((high - low) * (train.values - train_stats.iloc[3, :].values) / (train_stats.iloc[7, :].values - train_stats.iloc[3, :].values)) + low
                validation = ((high - low) * (validation.values - train_stats.iloc[3, :].values) / (train_stats.iloc[7, :].values - train_stats.iloc[3, :].values)) + low
                test = ((high - low) * (test.values - train_stats.iloc[3, :].values) / (train_stats.iloc[7, :].values - train_stats.iloc[3, :].values)) + low

            elif norm_options["METHOD"] == NORM_Z_SCORE:

                train = (train.values - train_stats.iloc[1, :].values) / train_stats.iloc[2, :].values
                validation = (validation.values - train_stats.iloc[1, :].values) / train_stats.iloc[2, :].values
                test = (test.values - train_stats.iloc[1, :].values) / train_stats.iloc[2, :].values

            walks['WALK_{}'.format(walk)]['TRAIN'] = get_sequences(arr=train, window=lookback)
            padding = train[-lookback:]

            walks['WALK_{}'.format(walk)]['VALIDATION'] = get_sequences(arr=validation, window=lookback, padding=padding)
            if len(validation) > 0:
                padding = validation[:-lookback]

            walks['WALK_{}'.format(walk)]['TEST'] = get_sequences(arr=test, window=lookback, padding=padding)

    else:

        for walk in range(n_walks):

            train = walks['WALK_{}'.format(walk)]['TRAIN'].values
            walks['WALK_{}'.format(walk)]['TRAIN'] = get_sequences(arr=train, window=lookback)
            padding = train[:-lookback]

            validation = walks['WALK_{}'.format(walk)]['VALIDATION'].values
            walks['WALK_{}'.format(walk)]['VALIDATION'] = get_sequences(arr=validation, window=lookback, padding=padding)
            if len(validation) > 0:
                padding = validation[:-lookback]

            test = walks['WALK_{}'.format(walk)]['TEST'].values
            walks['WALK_{}'.format(walk)]['TEST'] = get_sequences(arr=test, window=lookback, padding=padding)

    return walks


def mdd(arr, window=252):

    # Calculate the max drawdown in the past window days for each day in the series.
    # Use min_periods=1 if you want to let the first 252 days data have an expanding window
    roll_max = arr.rolling(window=window, min_periods=1).max()
    daily_drawdown = arr / roll_max

    # Next we calculate the minimum (negative) daily drawdown in that window.
    # Again, use min_periods=1 if you want to allow the expanding window
    max_daily_drawdown = daily_drawdown.rolling(window=window, min_periods=1).min()
    return daily_drawdown, max_daily_drawdown
#%%


#%%

from keras import activations
import json
import os
import time
import tensorflow
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, BatchNormalization, Activation, Bidirectional
from tensorflow.keras.callbacks import LearningRateScheduler
from sklearn.model_selection import ParameterGrid

#from stock_dataset import stock_dataset


class MultiModelFactory:

    def __init__(self, dataset_name=list(DATASETS_FILE_NAMES.keys())[0], stock_name_list=["IBM"], rem_features=[], lookback=60, split=(3, 1, 1), options=[], label="LR_IBM", norm_options={"METHOD": NORM_MIN_MAX, "HIGH_LOW": (0, 1)}):

        self.lookback = lookback
        self.split = split

        pre_processing_options = 0
        for opt in options:
            pre_processing_options = pre_processing_options | opt

        self.options = pre_processing_options
        self.label = label
        self.norm_options = norm_options
        self.dataset = pd.DataFrame()
        self.stock_name_list = stock_name_list
        for stock in self.stock_name_list:
            new_dataset = load_dataset(dataset_name=dataset_name, stock_name=stock)
            self.dataset = pd.concat([self.dataset, new_dataset],axis=1)#,join="inner")
        self.dataset = repair_dataset(dataset=self.dataset, nafix=CHK_FILL)
        self.walks = pre_processing_last(stock_name_list=self.stock_name_list,
                                         dataset=self.dataset,
                                         rem_features=rem_features,
                                         lookback=lookback,
                                         split=split,
                                         options=self.options,
                                         label=label,
                                         norm_options=norm_options)
        self.models = []


    def add_grid_search(self, models=[1], solvers=['adam'], learning_rates=[0.001], epochs=[1], batches=[32], lstm_units=[50], dense_layers=[0], conv_units=[0], learning_rate_decays=[1], learning_rate_steps=[1]):

        grid = dict(model=models,
                    solver=solvers,
                    learning_rate=learning_rates,
                    epochs=epochs,
                    batch=batches,
                    lstm_units=lstm_units,
                    dense_layers=dense_layers,
                    conv_units=conv_units,
                    learning_rate_decay=learning_rate_decays,
                    learning_rate_step=learning_rate_steps)

        grid = list(ParameterGrid(grid))

        self.models.extend(grid)

    def print_grid_search(self):

        for model in self.models:

            print("Model: ", model["model"],
                  "Solver: ", model["solver"],
                  "Learning Rate: ", model["lr"],
                  "Epochs: ", model["epochs"],
                  "Batch: ", model["batch"],
                  "Units in LSTM Layer: ", model["lstm_units"],
                  "N. of Dense Layer: ", model["dense_layers"],
                  "Units in Dense Layer: ", model["conv_units"])

    def create_nn(self, input_shape, version, dense_layers, conv_units, lstm_units, learning_rate):

        regressor = Sequential()

        if version == 0:

            regressor.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape)))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.Activation(activations.relu))
            regressor.add(Dropout(0.3))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(Dropout(0.2))

            # Adding a third LSTM
            regressor.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))
            regressor.add(Dropout(0.2))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(Dropout(0.2))

        elif version == 1:

            regressor.add(LSTM(units=lstm_units, input_shape=input_shape))
            regressor.add(Dropout(0.4))


        elif version == 2:

            regressor.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a third LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(Dropout(0.2))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(Dropout(0.2))

        elif version == 3:

            regressor.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a third LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

        elif version == 4:

            regressor.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))
            regressor.add(Dropout(0.3))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(Dropout(0.3))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(Dropout(0.3))

        # Output Layer
        for _ in range(dense_layers):

            regressor.add(Dense(units=512, activation='tanh'))


        regressor.add(Dense(units=1))

        mape = tensorflow.keras.metrics.MeanAbsolutePercentageError(name='mape')
        rmse = tensorflow.keras.metrics.RootMeanSquaredError(name='rmse')
        mse = tensorflow.keras.metrics.MeanSquaredError(name='mse')
        mae = tensorflow.keras.metrics.MeanAbsoluteError(name='mae')
        # mre = tensorflow.keras.metrics.MeanRelativeError(name='mre')
        msle = tensorflow.keras.metrics.MeanSquaredLogarithmicError(name='msle')
        csm = tensorflow.keras.metrics.CosineSimilarity(name='csm')
        lce = tensorflow.keras.metrics.LogCoshError(name='lce')

        # Compile RNN
        regressor.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate), loss=tensorflow.keras.losses.MeanSquaredError(), metrics=[mape, rmse, mse, mae, msle, csm, lce])

        return regressor

    def grid_search(self, result_path="./grid_search_results", data="VALIDATION"):

        if not os.path.exists(result_path):

            os.mkdir(result_path)

        results = np.zeros(7)
        histories = []
        count = 1
        countModel = 1

        start = time.time()

        for model in self.models:

            def lr_scheduler(epoch, lr):

                decay_rate = model['learning_rate_decay']
                decay_step = model['learning_rate_step']

                if epoch % decay_step == 0 and epoch:

                    return lr * pow(decay_rate, epoch // decay_step)

                return lr

            callbacks = [LearningRateScheduler(lr_scheduler, verbose=1)]

            for walk in range(self.walks['N_WALKS']):

                walk_data = self.walks['WALK_{}'.format(walk)]

                regressor = self.create_nn(input_shape=(walk_data['TRAIN'][0].shape[1], walk_data['TRAIN'][0].shape[2]),
                                           version=model['model'],
                                           dense_layers=model['dense_layers'],
                                           conv_units=model['conv_units'],
                                           lstm_units=model['lstm_units'],
                                           learning_rate=model['learning_rate'])

                print("\nmodel:", count, "/", len(self.models) * self.walks['N_WALKS'], "\nwalk:", walk, "\n")
                histories.append(regressor.fit(x=walk_data['TRAIN'][0],
                                               y=walk_data['TRAIN'][1],
                                               epochs=model['epochs'],
                                               batch_size=model['batch'],
                                               validation_data=walk_data['VALIDATION'],
                                               callbacks=callbacks))

                walk_data['REGRESSOR'] = regressor
                count += 1

            model_path = os.path.join(result_path, 'model_{}'.format(countModel))

            self.evaluate(data=data, result_path=model_path)

            json.dump(model, open(os.path.join(model_path + "_" + str(data), 'params.json'), 'w'))

            results = np.vstack((results, self.walks['RESULTS']['METRICS'].values))
            countModel += 1

        elapsed = time.time() - start
        hours = int(elapsed // 3600)
        mins = int((elapsed - (hours * 3600)) // 60)
        secs = int((elapsed - (hours * 3600) - (mins * 60)))

        if hours < 10: hours = "0" + str(hours)
        else:hours = str(hours)

        if mins < 10: mins = "0" + str(mins)
        else:mins = str(mins)

        if secs < 10: secs = "0" + str(secs)
        else: secs = str(secs)

        fp = open(os.path.join(result_path, 'elapsed.txt'), 'w')
        fp.write("Done in {}:{}:{}".format(hours, mins, secs))

        print()
        print("Done in {}:{}:{}".format(hours, mins, secs))
        print("All results:")
        results = pd.DataFrame(data=results[1:], columns=['MAPE', 'RMSE', 'MSE', 'MAE', 'MSLE', 'CSM', 'LCE'])
        results.to_csv(path_or_buf=os.path.join(result_path, 'all_models_overall.csv'), sep='\t')
        print(results)

    def fit(self):

        pass

    def evaluate(self, data='TEST', result_path='./evaluate_results'):

        result_path = result_path + "_" + data

        if not os.path.exists(result_path):

            os.mkdir(result_path)

        self.walks['RESULTS'] = {}
        self.walks['RESULTS']['METRICS'] = pd.DataFrame(data=np.asarray([[0, 0, 0, 0, 0, 0, 0]]), columns=['MAPE', 'RMSE', 'MSE', 'MAE', 'MSLE', 'CSM', 'LCE'])

        n_walks = self.walks['N_WALKS']
        walk_path = os.path.join(result_path, 'walk')

        print('Evaluating regressor on {} set:'.format(data))
        for walk in range(n_walks):

            walk_data = self.walks['WALK_{}'.format(walk)]
            self.evaluate_walk(walk=walk, data=data, result_path=walk_path)

            print("Walk {} results: ".format(walk))
            print(walk_data['RESULTS']['METRICS'])
            print()

            self.walks['RESULTS']['METRICS'] += walk_data['RESULTS']['METRICS'] / self.walks['N_WALKS']

        self.walks['RESULTS']['METRICS'].to_csv(path_or_buf=os.path.join(result_path, 'overall.csv'), sep='\t')

        print("Overall results: ")
        print(self.walks['RESULTS']['METRICS'])
        print()

    def evaluate_walk(self, walk, data='TEST', result_path='./'):

        result_path = result_path + "_" + str(walk)

        if not os.path.exists(result_path):

            os.mkdir(result_path)

        walk_data = self.walks['WALK_{}'.format(walk)]
        regressor = walk_data['REGRESSOR']

        pred = regressor.predict(x=walk_data[data][0])
        y = walk_data[data][1]

        walk_data['RESULTS'] = {}
        walk_data['RESULTS']['PRED'] = pred
        walk_data['RESULTS']['METRICS'] = metrics(y=y, pred=pred)

        walk_data['RESULTS']['METRICS'].to_csv(path_or_buf=os.path.join(result_path, 'walk_{}.csv'.format(walk)), sep='\t')

        self.plot_prediction_walk(walk=walk, data=data, result_path=result_path)

        return

    def predict(self):

        pass

    def plot_prediction_walk(self, walk, data='TEST', result_path='./'):

        walk_data = self.walks['WALK_{}'.format(walk)]
        y = walk_data[data][1]
        pred = walk_data['RESULTS']['PRED']

        if self.norm_options["METHOD"] == NORM_MIN_MAX:
            print("NORM_MIN_MAX")

            low = self.norm_options["HIGH_LOW"][0]
            high = self.norm_options["HIGH_LOW"][1]

            #maxP = walk_data['STD_PARAMS']['MAX'][0]
            #minP = walk_data['STD_PARAMS']['MIN'][0]


            #y = y * (maxP-minP) / (high-low) - low*(maxP-minP) + minP
            #pred = pred * (maxP-minP) / (high-low) - low*(maxP-minP) + minP

            y = (((y - low) / (high - low)) * (walk_data['STD_PARAMS']['MAX'][0] - walk_data['STD_PARAMS']['MIN'][0]) + walk_data['STD_PARAMS']['MIN'][0])
            pred = (((pred - low) / (high - low)) * (walk_data['STD_PARAMS']['MAX'][0] - walk_data['STD_PARAMS']['MIN'][0]) + walk_data['STD_PARAMS']['MIN'][0])

        elif self.norm_options["METHOD"] == NORM_Z_SCORE:

            y = (y * walk_data["STD_PARAMS"]["STD"]) + walk_data["STD_PARAMS"]["MEAN"]
            pred = (pred * walk_data["STD_PARAMS"]["STD"]) + walk_data["STD_PARAMS"]["MEAN"]
        print("OK")

        plt.figure()
        plt.plot(y, color='red', label='Real {} Stock Price'.format(self.stock_name_list[0]))
        plt.plot(pred, color='blue', label='Predicted {} Stock Price'.format(self.stock_name_list[0]))
        plt.title('{} Stock Price Prediction - walk {}'.format(self.stock_name_list[0], walk))
        plt.xlabel('Time')
        plt.ylabel('{} Stock Price'.format(self.stock_name_list[0]))
        plt.show()

        result_path = os.path.join(result_path, 'walk_{}.png'.format(walk))
        plt.savefig(fname=result_path, dpi=100)
        #global riscalato

        #riscalato.append(y)


#%%

from keras import activations
import json
import os
import time
import tensorflow
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, BatchNormalization, Activation, Bidirectional
from tensorflow.keras.callbacks import LearningRateScheduler
from sklearn.model_selection import ParameterGrid

#from stock_dataset import stock_dataset


class ModelFactory:

    def __init__(self, dataset_name=list(DATASETS_FILE_NAMES.keys())[0], stock_name="IBM", rem_features=[], lookback=60, split=(3, 1, 1), options=[], label="Close", norm_options={"METHOD": NORM_MIN_MAX, "HIGH_LOW": (0, 1)}):

        self.lookback = lookback
        self.split = split

        pre_processing_options = 0
        for opt in options:
            pre_processing_options = pre_processing_options | opt

        self.options = pre_processing_options
        self.label = label
        self.norm_options = norm_options

        self.stock_name = stock_name
        self.dataset = load_dataset(dataset_name=dataset_name, stock_name=self.stock_name)
        self.dataset = repair_dataset(dataset=self.dataset, nafix=CHK_FILL)
        self.walks = pre_processing(dataset=self.dataset, rem_features=rem_features, lookback=lookback, split=split, options=self.options, label=label, norm_options=norm_options)
        self.models = []

    def add_grid_search(self, models=[1], solvers=['adam'], learning_rates=[0.001], epochs=[1], batches=[32], lstm_units=[50], dense_layers=[0], conv_units=[0], learning_rate_decays=[1], learning_rate_steps=[1]):

        grid = dict(model=models,
                    solver=solvers,
                    learning_rate=learning_rates,
                    epochs=epochs,
                    batch=batches,
                    lstm_units=lstm_units,
                    dense_layers=dense_layers,
                    conv_units=conv_units,
                    learning_rate_decay=learning_rate_decays,
                    learning_rate_step=learning_rate_steps)

        grid = list(ParameterGrid(grid))

        self.models.extend(grid)

    def print_grid_search(self):

        for model in self.models:

            print("Model: ", model["model"],
                  "Solver: ", model["solver"],
                  "Learning Rate: ", model["lr"],
                  "Epochs: ", model["epochs"],
                  "Batch: ", model["batch"],
                  "Units in LSTM Layer: ", model["lstm_units"],
                  "N. of Dense Layer: ", model["dense_layers"],
                  "Units in Dense Layer: ", model["conv_units"])

    def create_nn(self, input_shape, version, dense_layers, conv_units, lstm_units, learning_rate):

        regressor = Sequential()

        if version == 0:

            regressor.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape)))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.Activation(activations.relu))
            regressor.add(Dropout(0.3))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(Dropout(0.2))

            # Adding a third LSTM
            regressor.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))
            regressor.add(Dropout(0.2))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(Dropout(0.2))

        elif version == 1:

            regressor.add(LSTM(units=lstm_units, input_shape=input_shape))
            regressor.add(Dropout(0.4))


        elif version == 2:

            regressor.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a third LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(Dropout(0.2))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(Dropout(0.2))

        elif version == 3:

            regressor.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a third LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(BatchNormalization())
            regressor.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

        elif version == 4:

            regressor.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))
            regressor.add(Dropout(0.3))

            # Adding a second LSTM
            regressor.add(LSTM(units=lstm_units, return_sequences=True))
            regressor.add(Dropout(0.3))

            # Adding a fourth LSTM
            regressor.add(LSTM(units=lstm_units))
            regressor.add(Dropout(0.3))

        # Output Layer
        for _ in range(dense_layers):

            regressor.add(Dense(units=512, activation='tanh'))


        regressor.add(Dense(units=1))

        mape = tensorflow.keras.metrics.MeanAbsolutePercentageError(name='mape')
        rmse = tensorflow.keras.metrics.RootMeanSquaredError(name='rmse')
        mse = tensorflow.keras.metrics.MeanSquaredError(name='mse')
        mae = tensorflow.keras.metrics.MeanAbsoluteError(name='mae')
        # mre = tensorflow.keras.metrics.MeanRelativeError(name='mre')
        msle = tensorflow.keras.metrics.MeanSquaredLogarithmicError(name='msle')
        csm = tensorflow.keras.metrics.CosineSimilarity(name='csm')
        lce = tensorflow.keras.metrics.LogCoshError(name='lce')

        # Compile RNN
        regressor.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate), loss=tensorflow.keras.losses.MeanSquaredError(), metrics=[mape, rmse, mse, mae, msle, csm, lce])

        return regressor

    def grid_search(self, result_path="./grid_search_results", data="VALIDATION"):

        if not os.path.exists(result_path):

            os.mkdir(result_path)

        results = np.zeros(7)
        histories = []
        count = 1
        countModel = 1

        start = time.time()

        for model in self.models:

            def lr_scheduler(epoch, lr):

                decay_rate = model['learning_rate_decay']
                decay_step = model['learning_rate_step']

                if epoch % decay_step == 0 and epoch:

                    return lr * pow(decay_rate, epoch // decay_step)

                return lr

            callbacks = [LearningRateScheduler(lr_scheduler, verbose=1)]

            for walk in range(self.walks['N_WALKS']):

                walk_data = self.walks['WALK_{}'.format(walk)]

                regressor = self.create_nn(input_shape=(walk_data['TRAIN'][0].shape[1], walk_data['TRAIN'][0].shape[2]),
                                           version=model['model'],
                                           dense_layers=model['dense_layers'],
                                           conv_units=model['conv_units'],
                                           lstm_units=model['lstm_units'],
                                           learning_rate=model['learning_rate'])

                print("\nmodel:", count, "/", len(self.models) * self.walks['N_WALKS'], "\nwalk:", walk, "\n")
                histories.append(regressor.fit(x=walk_data['TRAIN'][0],
                                               y=walk_data['TRAIN'][1],
                                               epochs=model['epochs'],
                                               batch_size=model['batch'],
                                               validation_data=walk_data['VALIDATION'],
                                               callbacks=callbacks))

                walk_data['REGRESSOR'] = regressor
                count += 1

            model_path = os.path.join(result_path, 'model_{}'.format(countModel))

            self.evaluate(data=data, result_path=model_path)

            json.dump(model, open(os.path.join(model_path + "_" + str(data), 'params.json'), 'w'))

            results = np.vstack((results, self.walks['RESULTS']['METRICS'].values))
            countModel += 1

        elapsed = time.time() - start
        hours = int(elapsed // 3600)
        mins = int((elapsed - (hours * 3600)) // 60)
        secs = int((elapsed - (hours * 3600) - (mins * 60)))

        if hours < 10: hours = "0" + str(hours)
        else:hours = str(hours)

        if mins < 10: mins = "0" + str(mins)
        else:mins = str(mins)

        if secs < 10: secs = "0" + str(secs)
        else: secs = str(secs)

        fp = open(os.path.join(result_path, 'elapsed.txt'), 'w')
        fp.write("Done in {}:{}:{}".format(hours, mins, secs))

        print()
        print("Done in {}:{}:{}".format(hours, mins, secs))
        print("All results:")
        results = pd.DataFrame(data=results[1:], columns=['MAPE', 'RMSE', 'MSE', 'MAE', 'MSLE', 'CSM', 'LCE'])
        results.to_csv(path_or_buf=os.path.join(result_path, 'all_models_overall.csv'), sep='\t')
        print(results)

    def fit(self):

        pass

    def evaluate(self, data='TEST', result_path='./evaluate_results'):

        result_path = result_path + "_" + data

        if not os.path.exists(result_path):

            os.mkdir(result_path)

        self.walks['RESULTS'] = {}
        self.walks['RESULTS']['METRICS'] = pd.DataFrame(data=np.asarray([[0, 0, 0, 0, 0, 0, 0]]), columns=['MAPE', 'RMSE', 'MSE', 'MAE', 'MSLE', 'CSM', 'LCE'])

        n_walks = self.walks['N_WALKS']
        walk_path = os.path.join(result_path, 'walk')

        print('Evaluating regressor on {} set:'.format(data))
        for walk in range(n_walks):

            walk_data = self.walks['WALK_{}'.format(walk)]
            self.evaluate_walk(walk=walk, data=data, result_path=walk_path)

            print("Walk {} results: ".format(walk))
            print(walk_data['RESULTS']['METRICS'])
            print()

            self.walks['RESULTS']['METRICS'] += walk_data['RESULTS']['METRICS'] / self.walks['N_WALKS']

        self.walks['RESULTS']['METRICS'].to_csv(path_or_buf=os.path.join(result_path, 'overall.csv'), sep='\t')

        print("Overall results: ")
        print(self.walks['RESULTS']['METRICS'])
        print()

    def evaluate_walk(self, walk, data='TEST', result_path='./'):

        result_path = result_path + "_" + str(walk)

        if not os.path.exists(result_path):

            os.mkdir(result_path)

        walk_data = self.walks['WALK_{}'.format(walk)]
        regressor = walk_data['REGRESSOR']

        pred = regressor.predict(x=walk_data[data][0])
        y = walk_data[data][1]

        walk_data['RESULTS'] = {}
        walk_data['RESULTS']['PRED'] = pred
        walk_data['RESULTS']['METRICS'] = metrics(y=y, pred=pred)

        walk_data['RESULTS']['METRICS'].to_csv(path_or_buf=os.path.join(result_path, 'walk_{}.csv'.format(walk)), sep='\t')

        self.plot_prediction_walk(walk=walk, data=data, result_path=result_path)

        return

    def predict(self):

        pass

    def plot_prediction_walk(self, walk, data='TEST', result_path='./'):

        walk_data = self.walks['WALK_{}'.format(walk)]
        y = walk_data[data][1]
        pred = walk_data['RESULTS']['PRED']

        if self.norm_options["METHOD"] == NORM_MIN_MAX:
            print("NORM_MIN_MAX")

            low = self.norm_options["HIGH_LOW"][0]
            high = self.norm_options["HIGH_LOW"][1]

            #maxP = walk_data['STD_PARAMS']['MAX'][0]
            #minP = walk_data['STD_PARAMS']['MIN'][0]


            #y = y * (maxP-minP) / (high-low) - low*(maxP-minP) + minP
            #pred = pred * (maxP-minP) / (high-low) - low*(maxP-minP) + minP

            y = (((y - low) / (high - low)) * (walk_data['STD_PARAMS']['MAX'][0] - walk_data['STD_PARAMS']['MIN'][0]) + walk_data['STD_PARAMS']['MIN'][0])
            pred = (((pred - low) / (high - low)) * (walk_data['STD_PARAMS']['MAX'][0] - walk_data['STD_PARAMS']['MIN'][0]) + walk_data['STD_PARAMS']['MIN'][0])

        elif self.norm_options["METHOD"] == NORM_Z_SCORE:

            y = (y * walk_data["STD_PARAMS"]["STD"]) + walk_data["STD_PARAMS"]["MEAN"]
            pred = (pred * walk_data["STD_PARAMS"]["STD"]) + walk_data["STD_PARAMS"]["MEAN"]
        print("OK")

        plt.figure()
        plt.plot(y, color='red', label='Real {} Stock Price'.format(self.stock_name))
        plt.plot(pred, color='blue', label='Predicted {} Stock Price'.format(self.stock_name))
        plt.title('{} Stock Price Prediction - walk {}'.format(self.stock_name, walk))
        plt.xlabel('Time')
        plt.ylabel('{} Stock Price'.format(self.stock_name))
        plt.show()

        result_path = os.path.join(result_path, 'walk_{}.png'.format(walk))
        plt.savefig(fname=result_path, dpi=100)
        #global riscalato

        #riscalato.append(y)

#%%

def metrics(y, pred):

    mape = tensorflow.keras.metrics.MeanAbsolutePercentageError()
    rmse = tensorflow.keras.metrics.RootMeanSquaredError()
    mse = tensorflow.keras.metrics.MeanSquaredError()
    mae = tensorflow.keras.metrics.MeanAbsoluteError()
    msle = tensorflow.keras.metrics.MeanSquaredLogarithmicError()
    csm = tensorflow.keras.metrics.CosineSimilarity()
    lce = tensorflow.keras.metrics.LogCoshError()

    mape.update_state(y, pred)
    rmse.update_state(y, pred)
    mse.update_state(y, pred)
    mae.update_state(y, pred)
    msle.update_state(y, pred)
    csm.update_state(y, pred)
    lce.update_state(y, pred)

    # print("MAPE:", mape.result().numpy())
    # print("RMSE:", rmse.result().numpy())
    # print("MSE:", mse.result().numpy())
    # print("MAE:", mae.result().numpy())
    # print("MSLE:", msle.result().numpy())
    # print("CSM:", csm.result().numpy())
    # print("LCE:", lce.result().numpy())

    columns = ['MAPE', 'RMSE', 'MSE', 'MAE', 'MSLE', 'CSM', 'LCE']
    data = np.asarray([[mape.result().numpy(), rmse.result().numpy(), mse.result().numpy(), mae.result().numpy(), msle.result().numpy(), csm.result().numpy(), lce.result().numpy()]])

    return pd.DataFrame(data=data, columns=columns)

#%%

stock_name = None
rem_features = ["High", "Low", "Volume", "Open","Close"]
lookback = 60
split = (3, 1, 1)
high_low = (0.01, 1)
pre_processing_options = [PRE_NORMALIZE,
                          PRE_INCLUDE_LR,
                          PRE_INCLUDE_TI]

norm_options = {

    "METHOD": NORM_MIN_MAX, #NORM_Z_SCORE
    "HIGH_LOW": high_low

}

# options = 0
# for opt in pre_processing_options:
#      options = options | opt
#
# dataset = stock_dataset.load_dataset(stock_name=stock_name)
# stock_dataset.check_dataset(dataset=dataset)
# dataset = stock_dataset.repair_dataset(dataset=dataset, nafix=stock_dataset.CHK_FILL)
#
# stock_dataset.plot_stock(dataset=dataset, stock_name=stock_name, variable_name='Open', split=(3, 1, 1))

# dataset = stock_dataset.pre_processing(dataset=dataset, rem_features=rem_features, lookback=lookback, split=(3, 1, 1), options=options, label='LR')
stocks = ["IBM","AMZN", "AAPL","MSFT"]#, "GOOGLE"]
mmf = MultiModelFactory(stock_name_list=stocks,rem_features=rem_features, lookback=lookback, split=split, options=pre_processing_options, label="LR_IBM", norm_options=norm_options)
mmf.add_grid_search(models=[1], epochs=[50], batches=[32], learning_rates=[0.001], learning_rate_steps=[10], learning_rate_decays=[0.90], dense_layers=[1], lstm_units=[64])
mmf.grid_search(data='VALIDATION')
#%%

mmf.evaluate(data='TEST')
#%%

mmf.dataset

#%%
walk_data = mf.walks["WALK_0"]
close = close["Close"]

plot_prediction_Close(close,walk_data["REGRESSOR"],walk_data["TEST"][0][0],walk_data["TEST"][0][1],9)
#%%

def plot_prediction_Close(real,regressor,X_test_set, y_test_set,n_walk,sc=None):
    def plot_prediction(name, y_test, pred):
      plt.plot(y_test, color = 'red', label = 'Real {} Stock Price'.format(name))
      plt.plot(pred, color = 'blue', label = 'Predicted {} Stock Price'.format(name))
      plt.title('{} Stock Price Prediction'.format(name))
      plt.xlabel('Time')
      plt.ylabel('{} Stock Price'.format(name))
      plt.show()

    real=real[-252:]
    real_one_day_before = real[:-1]
    i=7
    prediction = regressor.predict(X_test_set[i])

    if sc is not None:
        prediction = sc.inverse_transform(np.hstack((prediction,
                                                     np.zeros((len(X_test_set[i]), X_test_set[0].shape[2] - 1)))))

        y = y_test_set[i].reshape(-1, 1)
        y = sc.inverse_transform(np.hstack((y, np.zeros((len(y), X_test_set[0].shape[2] - 1)))))
        plot_prediction("name", real[1:], prediction[:, 0]*real_one_day_before+real_one_day_before)
        plot_prediction("name", y[:, 0], prediction[:, 0])
    else:
        plot_prediction("name", real[1:], prediction * real_one_day_before + real_one_day_before)
        for j in range(len(prediction[:10])):
            print(prediction[j], prediction[j] * real_one_day_before[j] + real_one_day_before[j],real[j+1],y_test_set[i])

    return prediction, y
